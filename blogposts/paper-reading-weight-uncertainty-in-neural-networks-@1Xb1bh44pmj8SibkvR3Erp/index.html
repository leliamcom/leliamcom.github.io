<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=css/global.css rel=stylesheet> <link href=manifest.json rel=manifest> <link href=favicon.ico rel=icon type=image/png> <link href=client/main.963202101.css rel=stylesheet><link href=client/[slug].2bc29ed5.css rel=stylesheet><link href=client/app.c6e68679.css rel=stylesheet><link href=client/Nav.fa717561.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Paper reading - Weight Uncertainty in Neural Networks</title> <meta content="Author: Liam, Date: 24 May, 2015\n Backpropagation, is a well known learning algorithm in neural network. In the algorithm,
the weight calculated is based on the out put of the result. To prevent overfitting and
introduce more uncertainty, its often comes with L1 and L2 regularization.
Weights with greater uncertainty introduce more variability into the decisions made by the
network, leading naturally to exploration
" name=description> <meta content=article name=og:type> <meta content=https://leliam.com/blogposts/paper-reading-weight-uncertainty-in-neural-networks-@1Xb1bh44pmj8SibkvR3Erp name=og:url> <meta content="Paper reading - Weight Uncertainty in Neural Networks" name=og:title> <meta content="Backpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\nWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\n" name=og:description><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <div style="min-height:calc(100vh - 48px)"> <nav aria-label="main navigation" class="svelte-1dif8ux navbar" role=navigation> <div class=navbar-brand> <a href=/ class="svelte-1dif8ux navbar-item" style=margin:5px><div class="is-4 subtitle">Le Liam</div></a> <a href=/ class="svelte-1dif8ux navbar-item">Home</a> <a href=/tags class="svelte-1dif8ux navbar-item">Tags</a> <a href=/blogposts class="svelte-1dif8ux navbar-item is-active">Blogposts</a> <a href=/contact class="svelte-1dif8ux navbar-item">Contact</a> </div> </nav> <main> <div class="container is-fluid"> <div class=media style="margin:25px 0"> <div class=media-left> <figure class="image is-48x48"> <img alt="Author avatar" loading=lazy src=/assets/32x32_avatar.jpg class=is-rounded> </figure> </div> <div class=media-content> <p class="is-4 title">Liam</p> <p class="subtitle is-6">24 May, 2015</p> </div> </div> <div style="margin:0 0 25px"> <img alt="blogpost banner" loading=lazy src=/assets/2015052300.png style="margin:0 auto;display:block"> </div> <h3 class=title>Paper reading - Weight Uncertainty in Neural Networks</h3> <div class="content svelte-nn94v0"> <p>This <a href=http://arxiv.org/pdf/1505.05424.pdf>paper</a> is published by Google DeepMind.</p> <h2>Background</h2> <p>Backpropagation, is a well known learning algorithm in neural network. In the algorithm, the weight calculated is based on the out put of the result. To prevent overfitting and introduce more uncertainty, its often comes with L1 and L2 regularization.</p> <p>Weights with greater uncertainty introduce more variability into the decisions made by the network, leading naturally to exploration</p> <h2>Reading</h2> <p>This article introduced a new regularization method called Bayes by Backprop.</p> <p>Instead of a fixed value, they view neural network as a probabilistic model.</p> <p><img alt="No-Drop vs DropOut vs DropConnect" loading=lazy src=/assets/2015052400.png title="No-Drop vs DropOut vs DropConnect"></p> <p>In Dropout or DropConnect, randomly selected activations or weights are set to zero. However in Bayes by Backprop, the activation is set based on its probability. When the dataset is big enough, its similar to the usual backpropagation algorithm, with more regularization.</p> <h2>Result</h2> <ol> <li>When classifying MNIST digits, performance from Bayes by Backprop(1.34%) is comparable to that of Dropout(~=1.3%), although each iteration of Bayes by Backprop is more expensive than Dropout – around two times slower).</li> <li>In MNIST digits, Dropconnect(1.2% test error) perform better than Bayes by Backprop.</li> </ol> <h2>Personal Thought</h2> <p>This paper comparison based on MNIST test error is not accurate enough, we should compare its false positive result with human eye classification - as some of MNIST labelling is arguable.</p> <p>Bayes by Backprop might achieve higher performance in specific situation.</p> </div> </div> </main> </div> <footer style=height:48px;padding:10px> <div class="content has-text-centered"> <a href=/privacy-policy>privacy policy</a> · ©2018-2020 Le Liam · <a href=/terms-of-service>TOS</a> </div> </footer></div> <script crossorigin=anonymous src="https://polyfill.io/v3/polyfill.min.js?features=fetch%2CPromise"></script> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{slug:"paper-reading-weight-uncertainty-in-neural-networks-@1Xb1bh44pmj8SibkvR3Erp",title:"Paper reading - Weight Uncertainty in Neural Networks",description:"\u003Cp\u003EBackpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\u003C\u002Fp\u003E\n\u003Cp\u003EWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\u003C\u002Fp\u003E\n",banner:"\u002Fassets\u002F2015052300.png",author:{avatar:"\u002Fassets\u002F32x32_avatar.jpg",name:"Liam"},data:"\u003Cp\u003EThis \u003Ca href=\"http:\u002F\u002Farxiv.org\u002Fpdf\u002F1505.05424.pdf\"\u003Epaper\u003C\u002Fa\u003E is published by Google DeepMind.\u003C\u002Fp\u003E\n\u003Ch2\u003EBackground\u003C\u002Fh2\u003E\n\u003Cp\u003EBackpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\u003C\u002Fp\u003E\n\u003Cp\u003EWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\u003C\u002Fp\u003E\n\u003Ch2\u003EReading\u003C\u002Fh2\u003E\n\u003Cp\u003EThis article introduced a new regularization method called Bayes by Backprop.\u003C\u002Fp\u003E\n\u003Cp\u003EInstead of a fixed value, they view neural network as a probabilistic model.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fassets\u002F2015052400.png\" alt=\"No-Drop vs DropOut vs DropConnect\" title=\"No-Drop vs DropOut vs DropConnect\" loading=\"lazy\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIn Dropout or DropConnect, randomly selected activations or weights are set to zero. However in\nBayes by Backprop, the activation is set based on its probability. When the dataset is big enough,\nits similar to the usual backpropagation algorithm, with more regularization.\u003C\u002Fp\u003E\n\u003Ch2\u003EResult\u003C\u002Fh2\u003E\n\u003Col\u003E\n\u003Cli\u003EWhen classifying MNIST digits, performance from Bayes by Backprop(1.34%) is comparable to that of\nDropout(~=1.3%), although each iteration of Bayes by Backprop is more expensive than Dropout –\naround two times slower).\u003C\u002Fli\u003E\n\u003Cli\u003EIn MNIST digits, Dropconnect(1.2% test error) perform better than Bayes by Backprop.\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch2\u003EPersonal Thought\u003C\u002Fh2\u003E\n\u003Cp\u003EThis paper comparison based on MNIST test error is not accurate enough, we should compare its false\npositive result with human eye classification - as some of MNIST labelling is arguable.\u003C\u002Fp\u003E\n\u003Cp\u003EBayes by Backprop might achieve higher performance in specific situation.\u003C\u002Fp\u003E\n",postedAt:"24 May, 2015",metaDescription:"Backpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\nWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\n"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.bb68c837.js"}catch(e){main="/client/legacy/client.138c9141.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <script> function trim(text, val) {
		return text.replace(new RegExp('^'+val+'+|'+val+'+$','g'), '');
	}
	if (document.title === "Not Found") {
		let paths = trim(window.location.pathname, "/").split("-@");
		console.log(paths);
		if (paths.length > 1) {
			fetch(`/blogposts/shortener-${paths[paths.length - 1]}.json`)
			.then((data) => {
				return data.json()
			})
			.then((data) => {
				window.location.replace(`/blogposts/${data.slug}`);
			})
			.catch((err) => {
				console.log(err);
			})
		}
	} </script> 