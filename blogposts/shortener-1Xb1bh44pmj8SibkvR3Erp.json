{"slug":"paper-reading-weight-uncertainty-in-neural-networks-@1Xb1bh44pmj8SibkvR3Erp","title":"Paper reading - Weight Uncertainty in Neural Networks","description":"<p>Backpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.</p>\n<p>Weights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration</p>\n","banner":"/assets/2015052300.png","author":{"avatar":"/assets/32x32_avatar.jpg","name":"Liam"},"data":"<p>This <a href=\"http://arxiv.org/pdf/1505.05424.pdf\">paper</a> is published by Google DeepMind.</p>\n<h2>Background</h2>\n<p>Backpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.</p>\n<p>Weights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration</p>\n<h2>Reading</h2>\n<p>This article introduced a new regularization method called Bayes by Backprop.</p>\n<p>Instead of a fixed value, they view neural network as a probabilistic model.</p>\n<p><img src=\"/assets/2015052400.png\" alt=\"No-Drop vs DropOut vs DropConnect\" title=\"No-Drop vs DropOut vs DropConnect\" loading=\"lazy\"></p>\n<p>In Dropout or DropConnect, randomly selected activations or weights are set to zero. However in\nBayes by Backprop, the activation is set based on its probability. When the dataset is big enough,\nits similar to the usual backpropagation algorithm, with more regularization.</p>\n<h2>Result</h2>\n<ol>\n<li>When classifying MNIST digits, performance from Bayes by Backprop(1.34%) is comparable to that of\nDropout(~=1.3%), although each iteration of Bayes by Backprop is more expensive than Dropout â€“\naround two times slower).</li>\n<li>In MNIST digits, Dropconnect(1.2% test error) perform better than Bayes by Backprop.</li>\n</ol>\n<h2>Personal Thought</h2>\n<p>This paper comparison based on MNIST test error is not accurate enough, we should compare its false\npositive result with human eye classification - as some of MNIST labelling is arguable.</p>\n<p>Bayes by Backprop might achieve higher performance in specific situation.</p>\n","postedAt":"24 May, 2015","metaDescription":"Backpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\nWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\n"}