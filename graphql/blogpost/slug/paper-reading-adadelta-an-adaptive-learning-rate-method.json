{"author":{"avatar":"/assets/32x32_avatar.jpg","name":"Liam","tagline":"Make people great again!"},"data":"\nThis [paper](http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf) was done by Matthew D. Zeiler while he was an intern at Google.\n\n## Introduction\n\nThe aim of many machine learning methods is to update a set of parameters $x$ in order to optimize an objective function $f(x)$.\nThis often involves some iterative procedure which applies changes to the parameters, $\\Delta{x}$ at each iteration of the algorithm.\nDenoting the parameters at the t-th iteration as $x_t$, this simple update rule becomes:\n\n$$\\Delta{x\\_t} = - \\eta{g\\_t}$$\n\n- $g_t$ is the gradient of the parameters at the t-th iteration\n- $Î·$ is a learning rate which controls how large of a step to take in the direction of the negative gradient\n\n\n## Purpose\n\nThe idea presented in this paper was derived from ADAGRAD in order to improve upon the two main drawbacks of the method:\n\n1. the continual decay of learning rates throughout training\n2. the need for a manually selected global learning rate.\n\n## SGD vs ADAGRAD vs ADADELTA\n\n- SGD: $$\\Delta{x\\_t} = \\rho{\\Delta{x\\_{t-1}}} - \\eta{g\\_t} $$\n  - where $\\rho$ is a constant controlling the decay of the previous parameter updates\n- ADAGRAD: $$\\Delta{x\\_t} = -{ {\\eta} \\over \\sqrt{\\sum\\_{T=1}^t g\\_{T}^2} } $$\n- ADADELTA: $$\\Delta{x\\_t} = -{ RMS\\[\\Delta{x}\\]\\_{t-1} \\over RMS\\[g\\]\\_t}g\\_t$$\n$$RMS[g]\\_t = \\sqrt{E[g^2]\\_t + \\epsilon}$$\n$$E[g^2]\\_t = \\rho{E[g^2]\\_{t-1} } + (1-\\rho)g_{t}^2$$\n  - where a constant $\\epsilon$ is added to better condition the denominator\n  - where $E[g^2]\\_t$ is expected value of gradient with power 2 at time t\n\n## Result\n\nCompared with SGD, ADAGRAD and MOMENTUM, normally ADADELTA has a convergence faster and has lower error rate.\n\n## Personal Thought\n\nHave tried ADADELTA and SGD. Although for each epoch ADADELTA takes longer time to compute, we just have to input (default value)\n$\\rho = 0.95$ and $\\epsilon = 1e^{-6}$ then it will learn very well. If use SGD, we have to fine tune the learning\nrate and the error rate is often bigger than ADADELTA.\n        ","id":"1f8f35e7-d29f-48a5-8c44-51eb5ed30492","bannerUrl":"/assets/2015061300.png","coverUrl":"/assets/2015061300.png","postedAt":"2015-06-12T16:00:00.000Z","tags":["machine-learning"],"title":"Paper reading - ADADELTA AN ADAPTIVE LEARNING RATE METHOD","typeCode":1}