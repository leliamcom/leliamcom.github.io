{"author":{"avatar":"/assets/32x32_avatar.jpg","name":"Liam","tagline":"Make people great again!"},"data":"\nThis [paper](http://arxiv.org/pdf/1505.05424.pdf) is published by Google DeepMind.\n\n## Background\n\nBackpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\n\nWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\n\n## Reading\n\nThis article introduced a new regularization method called Bayes by Backprop.\n\nInstead of a fixed value, they view neural network as a probabilistic model.\n\n![No-Drop vs DropOut vs DropConnect](http://localhost:9000/nghenglim-public/2015052400.png \"No-Drop vs DropOut vs DropConnect\")\n\nIn Dropout or DropConnect, randomly selected activations or weights are set to zero. However in\nBayes by Backprop, the activation is set based on its probability. When the dataset is big enough,\nits similar to the usual backpropagation algorithm, with more regularization.\n\n## Result\n\n1. When classifying MNIST digits, performance from Bayes by Backprop(1.34%) is comparable to that of\nDropout(~=1.3%), although each iteration of Bayes by Backprop is more expensive than Dropout â€“\naround two times slower).\n2. In MNIST digits, Dropconnect(1.2% test error) perform better than Bayes by Backprop.\n\n## Personal Thought\n\nThis paper comparison based on MNIST test error is not accurate enough, we should compare its false\npositive result with human eye classification - as some of MNIST labelling is arguable.\n\nBayes by Backprop might achieve higher performance in specific situation.\n    ","description":"\nBackpropagation, is a well known learning algorithm in neural network. In the algorithm,\nthe weight calculated is based on the out put of the result. To prevent overfitting and\nintroduce more uncertainty, its often comes with L1 and L2 regularization.\n\nWeights with greater uncertainty introduce more variability into the decisions made by the\nnetwork, leading naturally to exploration\n    ","id":"4037d219-f423-47f4-9f07-7e99e629891b","bannerUrl":"/assets/2015052300.png","coverUrl":"/assets/thumb-150-2015052300.png","postedAt":"2015-05-23T16:00:00.000Z","tags":["machine-learning"],"title":"Paper reading - Weight Uncertainty in Neural Networks","typeCode":1}